参考：

1. [https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2#grade-school-math-(gsm8k)](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2#grade-school-math-\(gsm8k\))
2. [http://arxiv.org/abs/2106.09685](http://arxiv.org/abs/2106.09685)

  

原论文

论文的假设：低秩模型（r=8）在当前任务下性能好，意味着高秩模型多出来的、不重合的信息是不重要的（甚至是噪声）。不严谨。不主要不等于无价值。

可以尝试只使用 r=64 模型中与 r=8 不重合的那些维度（比如第9到64维）来构建 LoRA 更新，看其效果如何。

核心优势在于，减小存储空间（主要在于参数和梯度而非模型本身）

  

LoRA缺陷

简单任务可以表现较好（WikiSQL MNLI-m SAMSum…)，除了因为特定领域相关的信息同时被低秩和高秩矩阵捕捉，还可能是因为通用基本的的映射方式存储在极少的几个奇异值最大的子空间中，因此论文中的热图展示出了一定程度的重合（重合的可能是通用的部分。通过分别和预训练模型对比，观察热图可以证明到底重合的是什么。）我感觉作者把一部分的基本通用信息也算进了领域特定信息中，导致对低秩近似的准确率判断“虚高”，从而损失了高维信息。我的判断基于“不同秩矩阵捕获信息的效率基本相同”的假设，参数矩阵的秩应该也会影响捕获信息的成分和效率，如何影响需要实验解释。

复杂的推理任务与高秩相比表现较差（1）。这部分信息可能储存在高维子空间中。高维子空间可能也与模型泛化能力相关。 低秩约束（特别是 r 非常小时）强迫模型只能关注并放大那些在训练数据中最显著、最强的模式。

不过在简单问题上，过高的维度更容易过拟合噪声。同时复杂规律只有高维能拟合。维度即能力。

如果能更好的解释什么类型或者复杂程度的任务和参数矩阵中的那些成分有关，就可以有针对性的微调。

学习率高时收敛不稳定。不知道跟低秩矩阵的什么特点有关。为了稳定微调，减小学习率，时间成本增加。更高的批量也没有让收敛更快（1）。但是可以并行训练多个模型。

  

改进方向：

1. 提升秩
2. 换一种分解
3. 改变初始化方式

  

预训练模型自己的训练数据可能很大程度上限制了不同领域的微调效果。

  

- **LoRA 的做法：** LoRA 并不直接计算 SVD（因为需要先知道理想的 ΔW，这正是我们想避免计算的）。相反，LoRA **直接将更新约束在低秩形式 BA**。通过最小化下游任务的损失函数来训练 A 和 B，梯度下降法会驱使 BA 趋向于一个**能够有效解决任务的低秩矩阵**。
- **逻辑联系：** 如果低秩假设成立（即理想的 ΔW 确实接近某个低秩矩阵），那么通过优化 A 和 B 找到的 BA 就能够很好地**近似**那个理想的（但未知的）低秩 ΔW_r 的效果。虽然 BA 不一定是数学上严格最优的秩 r 近似（即不一定等于 SVD 得到的 ΔW_r），但它是由任务损失驱动找到的一个**有效的**低秩解。训练过程找到了在 r 维限制下，对解决当前任务“足够好”的权重更新方向。

  

需求是，从小矩阵开始微调，之后变换成大矩阵。与矩阵分解相反，更像是矩阵合成。不过LoRA通过两个矩阵通过乘法合成在一起，以秩r内接（r是我们自己选的），选的太小导致信息损失，太大就占空间。 同样的思路，有其他的方法吧小矩阵变换到大矩阵上吗？

  

把重要的参数成分筛出来，除了数学方法（PCA，LoRA），还可以把原矩阵的pooling作为低维矩阵的初始化，

或者用“滑动”注意力（q=待学习的低维度参数矩阵，k=原参数矩阵的同size窗口），类比卷积滑动窗口，只不过计算的是注意力分数。先把原参数矩阵pooling到稍微小一点的矩阵减少计算量。

autoencoder似乎是一种更直接的办法：学习把高维参数矩阵到低维的恒等映射（targer = input），从而提炼出参数矩阵的”固有秩“。但跟“超网络”类似，可能有点大炮打苍蝇？不知道效率如何，而且在NLP领域应用较少（In fact, this simple autoencoder often ends up learning a low-dimensional representation very similar to PCA’s.）

Y. Qin, X. Wang, Y. Su, Y. Lin, N. Ding, J. Yi, W. Chen, Z. Liu, J. Li, L. Hou et al., “Exploring universal intrinsic task subspace via prompt tuning,” arXiv preprint arXiv:2110.07867, 2021.

  

比较

大部分改进方法的效果微乎其微，可能对setting极其敏感，极少数情况下才生成有更好的性能。LoRA和Houlsby Adapters在下游任务性能最稳健，LN方法极其简单而且metrics和训练时间上性能都不错。

在论文An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning中，选择性的只微调transformer部分组件，LN层的表现突出。

1. **LayerNorm 有效性的可能原因：高幅度的权重:**
    - 作者通过可视化不同模块的权重分布（Figure 1），发现 LayerNorm 层的权重呈现**双峰分布 (bimodal distribution)**，并且其中一个峰值具有**显著高于**其他模块（FFN, MHA）权重的**平均幅度和方差**。
    - **假设:** 这些高幅度的权重（或称为“离群值 Outliers”）在预训练过程中可能捕获了关键信息，并且在微调时对模型行为的调整起到了更重要的杠杆作用。
2. **离群值调优 (Outlier Tuning) 验证假设:**
    - 为了验证上述假设，作者设计了“离群值调优”实验：在每个 LayerNorm 模块中，只选择 n 个（n=4, 16, 64, 256）**距离均值最远（即幅度最大）**的权重进行微调，其余全部冻结。
    - 结果（Table 4）表明，即使只微调极少数（例如 n=4 或 n=16）的 LayerNorm 离群值权重（参数量 < 0.0004%），性能也远超冻结模型，并且显著优于随机选择同等数量参数。当 n 增大到 256 时（参数量 0.0056%），性能已经非常接近全量微调。
    - 这强有力地支持了 LayerNorm 的有效性很大程度上归功于其包含的**高幅度权重（离群值）**。

联系到之前看到过的，transformer中的冗余：从词嵌入范数的角度，LN抵消了上下文化，这不能说明这是好事还是坏事。但是可以说明归一化这一步起到了至关重要的枢纽作用，而且很有可能体现在特定任务的适应性微调上。

LN也许是名副其实的更重要的组件。

  

  

通过outlier切入，优化微调效果。